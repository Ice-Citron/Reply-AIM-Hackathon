{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873e61d",
   "metadata": {},
   "outputs": [],
   "source": "# !pip install openpipe-art==0.5.0 langchain-core tenacity datasets vllm faiss-cpu chromadb requests lxml numpy transformers torch gql==3.4.1 peft \n# !pip install langchain-core tenacity datasets vllm\n\n# ============================================================\n# PATH SETUP - Add project root to sys.path\n# ============================================================\nimport sys\nfrom pathlib import Path\n\n# Navigate up to project root from training/serverless/\nPROJECT_ROOT = Path().resolve().parent.parent\nsys.path.insert(0, str(PROJECT_ROOT))\n\nprint(f\"âœ… Project root: {PROJECT_ROOT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965eaa9e",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom secretsConfig import oaiKey, wandbKey, openRouterKey\n\n# Set environment variables for API keys\nos.environ[\"OPENAI_API_KEY\"] = oaiKey\nos.environ[\"WANDB_API_KEY\"] = wandbKey\nos.environ[\"OPENROUTER_API_KEY\"] = openRouterKey\n\n# Validate keys\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY is required for RULER functionality.\")\n\nif not os.environ.get(\"WANDB_API_KEY\"):\n    raise ValueError(\"WANDB_API_KEY is required for W&B.\")\n\nif not os.environ.get(\"OPENROUTER_API_KEY\"):\n    raise ValueError(\"OPENROUTER_API_KEY is required for Gemini judge.\")\n\nprint(\"âœ… API keys loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c3f51",
   "metadata": {},
   "outputs": [],
   "source": "# Import centralized paths\nfrom config.paths import LEGAL_XML_FILE, CHROMA_DB_PATH\n\n# Import RAG tools\nfrom rag_tools.semantic_search import FAISSSemanticSearch\nfrom rag_tools.keyword_search import keyword_search\nfrom rag_tools.read_document import read_document_part\n\n# Wrap tools with error handling\ndef keyword_search_tool(query: str, num: int = 5) -> str:\n    \"\"\"BM25 keyword search\"\"\"\n    try:\n        return keyword_search(str(LEGAL_XML_FILE), query, num)\n    except Exception as e:\n        error_msg = f\"[TOOL ERROR] keyword_search failed: {str(e)}\"\n        print(error_msg)\n        return error_msg\n\ndef semantic_search_tool(query: str, num: int = 5) -> str:\n    \"\"\"FAISS semantic search\"\"\"\n    try:\n        searcher = FAISSSemanticSearch(chroma_path=str(CHROMA_DB_PATH))\n        return searcher.search(query, num)\n    except Exception as e:\n        error_msg = f\"[TOOL ERROR] semantic_search failed: {str(e)}\"\n        print(error_msg)\n        return error_msg\n\ndef read_document_part_tool(part_id: str) -> str:\n    \"\"\"Read document part by ID\"\"\"\n    try:\n        if \" \" in part_id or len(part_id) > 100:\n            error_msg = f\"[INVALID PART_ID] '{part_id[:50]}...' is not a valid part_id format.\"\n            print(f\"[ERROR] Model tried to read invalid part_id: {part_id[:50]}...\")\n            return error_msg\n        return read_document_part(str(LEGAL_XML_FILE), part_id)\n    except Exception as e:\n        error_msg = f\"[TOOL ERROR] Failed to read document part: {str(e)}\"\n        print(f\"[ERROR] {error_msg}\")\n        return error_msg\n\nprint(\"âœ… RAG tools loaded with centralized paths\")\nprint(f\"   - Legal XML: {LEGAL_XML_FILE}\")\nprint(f\"   - ChromaDB: {CHROMA_DB_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e659f",
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\nimport random\n\nimport art\nfrom art.serverless.backend import ServerlessBackend\n\n# Import serverless configuration\nfrom config.training_config import BASE_MODEL, WANDB_CONFIG\nimport config_serverless\n\nload_dotenv()\nrandom.seed(42)\n\n# Print configuration\nconfig_serverless.print_config()\n\n# Get model config from serverless config\nmodel_config = config_serverless.get_model_config()\nprint(f\"ðŸ“¦ Creating model: {model_config['base_model']}\")\n\n# Create trainable model\nmodel = art.TrainableModel(**model_config)\n\n# Initialize serverless backend\nbackend = config_serverless.get_backend()\nprint(f\"ðŸ”§ Backend: W&B Serverless\")\n\n# Register the model\nawait model.register(backend)\nprint(f\"âœ… Model registered\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e83a1",
   "metadata": {},
   "outputs": [],
   "source": "from textwrap import dedent\nfrom pydantic import BaseModel, Field\nfrom openai import AsyncOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nimport art\n\n# Import MAX_TURNS from config\nfrom config.training_config import MAX_TURNS\n\n\nclass FinalAnswer(BaseModel):\n    answer: str\n    source_ids: list[str]\n\n\nclass LegalScenario(BaseModel):\n    id: str\n    question: str\n    gold_answer: str | None = None\n    gold_part_ids: list[str] | None = None\n\n\nclass LegalScenarioStep(BaseModel):\n    step: int\n    scenario: LegalScenario\n\n\nasync def rollout(model: art.Model, legal_scenario_step: LegalScenarioStep) -> art.Trajectory:\n    \"\"\"Execute one trajectory rollout\"\"\"\n    scenario = legal_scenario_step.scenario\n    \n    traj = art.Trajectory(\n        reward=0.0,\n        messages_and_choices=[],\n        metadata={\"scenario_id\": scenario.id, \"step\": legal_scenario_step.step},\n    )\n\n    system_prompt = dedent(\n        f\"\"\"\n        You are a legal research assistant that can search legal documents to answer questions.\n\n        You have access to the following tools:\n        - search_keyword(query: str, num: int) -> str\n        - search_semantic(query: str, num: int) -> str\n        - read_document_part(part_id: str) -> str\n\n        You may call one tool per turn, for up to {MAX_TURNS} turns.\n\n        When ready, give your final answer in this format:\n        <answer>\n        [your answer or \"I don't know\" if insufficient information]\n        <sources>\n        <source>doc_id</source>\n        </sources>\n        </answer>\n        \"\"\"\n    )\n\n    traj.messages_and_choices = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": scenario.question},\n    ]\n\n    # Define tools using centralized tool functions\n    def return_final_answer(answer: str, source_ids: list[str]) -> FinalAnswer:\n        return FinalAnswer(answer=answer, source_ids=source_ids)\n\n    tools = [keyword_search_tool, semantic_search_tool, read_document_part_tool, return_final_answer]\n    tools_by_name = {t.__name__: t for t in tools}\n    traj.tools = [convert_to_openai_tool(t) for t in tools]\n\n    client = AsyncOpenAI(\n        base_url=model.inference_base_url,\n        api_key=model.inference_api_key,\n    )\n\n    for _ in range(MAX_TURNS):\n        response = await client.chat.completions.create(\n            model=model.get_inference_name(),\n            temperature=1,\n            messages=traj.messages(),\n            tools=traj.tools,\n        )\n\n        response_message = response.choices[0].message\n        traj.messages_and_choices.append(response.choices[0])\n\n        if not response_message.tool_calls:\n            return traj\n\n        try:\n            import json\n            for tool_call in response_message.tool_calls:\n                tool_name = tool_call.function.name\n                if tool_name in tools_by_name:\n                    tool_args = json.loads(tool_call.function.arguments)\n                    result = tools_by_name[tool_name](**tool_args)\n                    traj.messages_and_choices.append({\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"name\": tool_name,\n                        \"content\": str(result),\n                    })\n\n                    if tool_name == \"return_final_answer\":\n                        return traj\n        except Exception as e:\n            print(f\"Tool error: {e}\")\n            return traj\n\n    return traj\n\n\nprint(\"âœ… Rollout function defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5c17",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom litellm import acompletion\n\n# Import centralized data path\nfrom config.paths import TRAINING_DATA_FILE\nfrom config.training_config import MAX_TURNS\n\n# Load training data from centralized path\nprint(f\"ðŸ“‚ Loading data from {TRAINING_DATA_FILE}...\")\nwith open(TRAINING_DATA_FILE, 'r') as f:\n    data = json.load(f)\n\n# Convert to LegalScenario objects\ntraining_scenarios = []\nfor item in data.get(\"items\", []):\n    for row in item.get(\"rows\", []):\n        training_scenarios.append(\n            LegalScenario(\n                id=str(row[\"row_index\"]),\n                question=row[\"question\"],\n                gold_answer=row.get(\"model_answer\", \"\"),\n                gold_part_ids=row.get(\"sources\", [])\n            )\n        )\n\nprint(f\"âœ… Loaded {len(training_scenarios)} scenarios\\n\")\n\n\n# Gemini judge with updated scoring criteria\nasync def gemini_judge(group: art.TrajectoryGroup) -> art.TrajectoryGroup:\n    \"\"\"Score trajectories with new criteria\"\"\"\n    trajectories = group.trajectories\n    \n    if len(trajectories) <= 1:\n        for traj in trajectories:\n            traj.reward = 0.0\n        return group\n\n    # Analyze trajectories\n    analyses = []\n    for i, traj in enumerate(trajectories):\n        messages = traj.messages()\n        final_answer = messages[-1].get(\"content\", \"\") if messages else \"\"\n\n        format_errors = []\n        num_searches = 0\n        num_turns = sum(1 for m in messages if isinstance(m, dict) and m.get(\"role\") == \"assistant\")\n\n        for msg in messages:\n            if isinstance(msg, dict) and msg.get(\"role\") == \"tool\":\n                content = msg.get(\"content\", \"\")\n                if \"search\" in msg.get(\"name\", \"\").lower():\n                    num_searches += 1\n                if any(err in content for err in [\"[TOOL ERROR]\", \"[INVALID PART_ID]\", \"[PART NOT FOUND]\"]):\n                    format_errors.append(content[:80])\n\n        analyses.append({\n            \"index\": i,\n            \"final_answer\": final_answer[:400],\n            \"num_turns\": num_turns,\n            \"num_searches\": num_searches,\n            \"has_format_error\": len(format_errors) > 0,\n        })\n\n    # Build judge prompt\n    analysis_text = \"\\\\n\".join([\n        f\"**Response {a['index']+1}:** {a['final_answer']}\\\\n\"\n        f\"  Turns: {a['num_turns']}/{MAX_TURNS}, Searches: {a['num_searches']}, \"\n        f\"Errors: {'YES' if a['has_format_error'] else 'NO'}\"\n        for a in analyses\n    ])\n\n    judge_prompt = f\"\"\"Score {len(trajectories)} legal research responses:\n\nRULES:\n- Correct answer: 1.0 to 2.0\n- \"I don't know\": 0.0 to 1.0 (GOOD - avoids hallucination)\n- Wrong answer: -1.0 to 0.0\n- Format errors: -2.0 to -1.0\n\n{analysis_text}\n\nReturn JSON: {{\"scores\": [{{\"base_score\": X, \"reasoning\": \"...\"}}, ...]}}\"\"\"\n\n    try:\n        response = await acompletion(\n            model=\"openrouter/google/gemini-2.5-flash\",\n            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n            api_key=os.environ[\"OPENROUTER_API_KEY\"],\n            max_tokens=500,\n        )\n\n        import re\n        result_text = response.choices[0].message.content.strip()\n        json_match = re.search(r'\\\\{[\\\\s\\\\S]*\\\\}', result_text)\n\n        if json_match:\n            result = json.loads(json_match.group())\n            base_scores = [item[\"base_score\"] for item in result[\"scores\"]]\n        else:\n            base_scores = [0.0] * len(trajectories)\n\n        # Apply efficiency bonuses\n        final_scores = []\n        for base_score, analysis in zip(base_scores, analyses):\n            score = float(base_score)\n            if not analysis['has_format_error']:\n                turn_eff = (MAX_TURNS - analysis['num_turns']) / MAX_TURNS\n                score += turn_eff * 0.2\n                if analysis['num_searches'] <= 2:\n                    score += 0.1\n            final_scores.append(round(score, 2))\n\n        for traj, score in zip(trajectories, final_scores):\n            traj.reward = float(score)\n\n        print(f\"  Scores: {final_scores}\")\n\n    except Exception as e:\n        print(f\"Judge error: {e}\")\n        for traj, analysis in zip(trajectories, analyses):\n            traj.reward = -1.5 if analysis['has_format_error'] else 0.5\n\n    return group\n\n\nprint(\"âœ… Judge function defined with updated scoring criteria\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f56c",
   "metadata": {},
   "outputs": [],
   "source": "from art.utils import iterate_dataset\nimport wandb\n\n# Import centralized config\nfrom config.training_config import WANDB_CONFIG\nimport config_serverless\n\n# Initialize W&B\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n# Get serverless config\ncfg = config_serverless.SERVERLESS_CONFIG\n\nrun = wandb.init(\n    project=WANDB_CONFIG[\"project\"],\n    config={\n        \"model\": cfg[\"base_model\"],\n        \"groups_per_step\": cfg[\"groups_per_step\"],\n        \"num_epochs\": cfg[\"num_epochs\"],\n        \"rollouts_per_group\": cfg[\"rollouts_per_group\"],\n        \"learning_rate\": cfg[\"learning_rate\"],\n        \"max_steps\": cfg[\"max_steps\"],\n        \"max_turns\": MAX_TURNS,\n    },\n)\n\nprint(\"ðŸš€ Starting training loop...\")\nprint(f\"ðŸ’» Model inference: W&B Serverless\")\nprint(f\"ðŸ“Š W&B Dashboard: {run.url}\")\nprint(f\"ðŸ·ï¸  Run: {run.name}\\\\n\")\n\n# Create training iterator\ntraining_iterator = iterate_dataset(\n    training_scenarios,\n    groups_per_step=cfg[\"groups_per_step\"],\n    num_epochs=cfg[\"num_epochs\"],\n    initial_step=0,\n)\n\nstep_count = 0\n\nfor batch in training_iterator:\n    print(f\"=== Step {step_count} | Epoch {batch.epoch} ===\")\n    \n    # Create trajectory groups\n    groups = [\n        art.TrajectoryGroup([\n            rollout(model, LegalScenarioStep(step=batch.step, scenario=scenario))\n            for _ in range(cfg[\"rollouts_per_group\"])\n        ])\n        for scenario in batch.items\n    ]\n    \n    # Gather trajectories\n    finished_groups = await art.gather_trajectory_groups(\n        groups,\n        pbar_desc=\"Rollouts\",\n        max_exceptions=cfg[\"rollouts_per_group\"] * len(batch.items),\n    )\n    \n    # Judge\n    judged_groups = [await gemini_judge(g) for g in finished_groups]\n    \n    # Log metrics\n    all_rewards = [t.reward for g in judged_groups for t in g.trajectories]\n    wandb.log({\n        \"step\": step_count,\n        \"avg_reward\": sum(all_rewards) / len(all_rewards),\n        \"max_reward\": max(all_rewards),\n        \"min_reward\": min(all_rewards),\n    })\n    \n    print(f\"  Rewards: avg={sum(all_rewards)/len(all_rewards):.3f}\")\n    \n    # Train\n    await model.delete_checkpoints()\n    await model.train(judged_groups, config=art.TrainConfig(learning_rate=cfg[\"learning_rate\"]))\n    \n    print(f\"âœ… Step {step_count} complete\\\\n\")\n    \n    step_count += 1\n    if step_count >= cfg[\"max_steps\"]:\n        break\n\nrun.finish()\nprint(\"ðŸŽ‰ Training complete!\")\nprint(f\"ðŸ“Š View results: {run.url}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b06c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}