{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpipe-art==0.5.0\n",
      "  Using cached openpipe_art-0.5.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting tenacity\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.11.2-cp38-abi3-manylinux1_x86_64.whl.metadata (18 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (2.32.5)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (2.1.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Collecting gql==3.4.1\n",
      "  Using cached gql-3.4.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting litellm==1.74.1 (from openpipe-art==0.5.0)\n",
      "  Using cached litellm-1.74.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting openai<=1.99.1,>=1.65.5 (from openpipe-art==0.5.0)\n",
      "  Using cached openai-1.99.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting typer>=0.15.2 (from openpipe-art==0.5.0)\n",
      "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Using cached weave-0.52.17-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting graphql-core<3.3,>=3.2 (from gql==3.4.1)\n",
      "  Using cached graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.6 (from gql==3.4.1)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql==3.4.1)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting click (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting httpx>=0.23.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /venv/main/lib/python3.12/site-packages (from litellm==1.74.1->openpipe-art==0.5.0) (3.1.4)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3.0.0,>=2.5.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tokenizers (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.1->openpipe-art==0.5.0) (2.1.5)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached rpds_py-0.29.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /venv/main/lib/python3.12/site-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /venv/main/lib/python3.12/site-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /venv/main/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (3.10)\n",
      "Requirement already satisfied: certifi in /venv/main/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting multidict>=4.0 (from yarl<2.0,>=1.6->gql==3.4.1)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.1 (from yarl<2.0,>=1.6->gql==3.4.1)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core)\n",
      "  Using cached langsmith-0.4.45-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /venv/main/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /venv/main/lib/python3.12/site-packages (from langchain-core) (6.0.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Collecting regex (from vllm)\n",
      "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting cachetools (from vllm)\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from vllm) (7.0.0)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Using cached blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting protobuf (from vllm)\n",
      "  Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi-0.121.3-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting prometheus_client>=0.18.0 (from vllm)\n",
      "  Using cached prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pillow in /venv/main/lib/python3.12/site-packages (from vllm) (11.0.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<1.4.0,>=1.3.0 (from vllm)\n",
      "  Using cached llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm)\n",
      "  Using cached outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.25 (from vllm)\n",
      "  Using cached xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /venv/main/lib/python3.12/site-packages (from vllm) (27.1.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Using cached gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.5 (from mistral_common[image]>=1.8.5->vllm)\n",
      "  Using cached mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.16.0 in /venv/main/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=77.0.3 in /venv/main/lib/python3.12/site-packages (from vllm) (80.9.0)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.12.2 (from vllm)\n",
      "  Using cached compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.20.0 (from vllm)\n",
      "  Using cached depyf-0.20.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Using cached cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting python-json-logger (from vllm)\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting scipy (from vllm)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Using cached pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Using cached cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Using cached setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Using cached openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting anthropic==0.71.0 (from vllm)\n",
      "  Using cached anthropic-0.71.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting model-hosting-container-standards<1.0.0 (from vllm)\n",
      "  Using cached model_hosting_container_standards-0.1.9-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Using cached numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached ray-2.52.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchaudio==2.9.0 (from vllm)\n",
      "  Using cached torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torchvision==0.24.0 (from vllm)\n",
      "  Using cached torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting xformers==0.0.33.post1 (from vllm)\n",
      "  Using cached xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting flashinfer-python==0.5.2 (from vllm)\n",
      "  Using cached flashinfer_python-0.5.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic==0.71.0->vllm)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting loguru (from compressed-tensors==0.12.2->vllm)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting astor (from depyf==0.20.0->vllm)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting apache-tvm-ffi<0.2,>=0.1 (from flashinfer-python==0.5.2->vllm)\n",
      "  Using cached apache_tvm_ffi-0.1.3-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python==0.5.2->vllm)\n",
      "  Using cached nvidia_cudnn_frontend-1.16.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cutlass-dsl>=4.2.1 (from flashinfer-python==0.5.2->vllm)\n",
      "  Using cached nvidia_cutlass_dsl-4.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py (from flashinfer-python==0.5.2->vllm)\n",
      "  Using cached nvidia_ml_py-13.580.82-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting tabulate (from flashinfer-python==0.5.2->vllm)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Using cached llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting jmespath (from model-hosting-container-standards<1.0.0->vllm)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting starlette>=0.49.1 (from model-hosting-container-standards<1.0.0->vllm)\n",
      "  Using cached starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting supervisor>=4.2.0 (from model-hosting-container-standards<1.0.0->vllm)\n",
      "  Using cached supervisor-4.3.0-py2.py3-none-any.whl.metadata (87 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /venv/main/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi_cli-0.0.16-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached rich_toolkit-0.16.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi_cloud_cli-0.5.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached sentry_sdk-2.45.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastar>=0.5.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastar-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Using cached pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting cuda-python>=12.8 (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "  Using cached cuda_python-13.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting cuda-bindings~=13.0.3 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "  Using cached cuda_bindings-13.0.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "  Using cached cuda_pathfinder-1.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting click (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /venv/main/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.2->openpipe-art==0.5.0)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting eval-type-backport (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached eval_type_backport-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting polyfile-weave (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached polyfile_weave-0.5.7-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting wandb>=0.17.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached wandb-0.23.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "INFO: pip is looking at multiple versions of gql[aiohttp,requests] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Using cached weave-0.52.16-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.14-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.11-py3-none-any.whl.metadata (27 kB)\n",
      "INFO: pip is looking at multiple versions of weave to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached weave-0.52.10-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.9-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.8-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /venv/main/lib/python3.12/site-packages (from weave>=0.51.51->openpipe-art==0.5.0) (1.6.0)\n",
      "  Using cached weave-0.52.7-py3-none-any.whl.metadata (27 kB)\n",
      "INFO: pip is still looking at multiple versions of weave to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached weave-0.52.6-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.5-py3-none-any.whl.metadata (27 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached weave-0.52.4-py3-none-any.whl.metadata (27 kB)\n",
      "INFO: pip is still looking at multiple versions of gql[aiohttp,requests] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached weave-0.52.3-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.2-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.52.1-py3-none-any.whl.metadata (27 kB)\n",
      "  Using cached weave-0.51.59-py3-none-any.whl.metadata (26 kB)\n",
      "  Using cached weave-0.51.56-py3-none-any.whl.metadata (26 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached weave-0.51.55-py3-none-any.whl.metadata (26 kB)\n",
      "  Using cached weave-0.51.54-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting emoji>=2.12.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Using cached weave-0.51.53-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uuid-utils>=0.9.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached uuid_utils-0.11.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Using cached weave-0.51.52-py3-none-any.whl.metadata (25 kB)\n",
      "  Using cached weave-0.51.51-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic[email]>=1.6.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic[email] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached pydantic-2.12.2-py3-none-any.whl.metadata (85 kB)\n",
      "  Using cached pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\n",
      "Collecting pydantic-extra-types[pycountry]>=2.10.5 (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Using cached pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic-extra-types[pycountry] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.11.1-cp38-abi3-manylinux1_x86_64.whl.metadata (18 kB)\n",
      "INFO: pip is still looking at multiple versions of pydantic[email] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Using cached openai_harmony-0.0.6-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "  Using cached openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "  Using cached openai_harmony-0.0.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached fastapi_cloud_cli-0.5.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached fastapi_cloud_cli-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.3.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.2.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.2.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.1.5-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.1.4-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Using cached fastapi_cloud_cli-0.1.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting httpx>=0.23.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fastapi[standard]>=0.115.0 (from vllm)\n",
      "  Using cached fastapi-0.121.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting starlette>=0.49.1 (from model-hosting-container-standards<1.0.0->vllm)\n",
      "  Using cached starlette-0.49.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting fastapi[standard]>=0.115.0 (from vllm)\n",
      "  Using cached fastapi-0.121.1-py3-none-any.whl.metadata (28 kB)\n",
      "  Using cached fastapi-0.121.0-py3-none-any.whl.metadata (28 kB)\n",
      "  Using cached fastapi-0.120.4-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /venv/main/lib/python3.12/site-packages (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0) (4.4.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting eval-type-backport (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached fastrlock-0.8.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Using cached cupy_cuda12x-13.5.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-9.2-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Using cached cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Using cached blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\n",
      "Collecting astor (from depyf==0.20.0->vllm)\n",
      "  Using cached astor-0.8.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Using cached zstandard-0.24.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Using cached zipp-3.22.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wandb>=0.17.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached wandb-0.22.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting nest-asyncio==1.6.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-1.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.18.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached watchfiles-0.18.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting emoji>=2.12.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached anyio-3.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-0.17.0-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting uuid-utils>=0.9.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Using cached uuid_utils-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Using cached watchfiles-0.16.1-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.7 kB)\n",
      "  Using cached watchfiles-0.16.0-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached uvloop-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached uvloop-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached uvloop-0.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Using cached uvloop-0.17.0.tar.gz (2.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached uvloop-0.16.0.tar.gz (2.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached uvloop-0.15.3.tar.gz (2.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached uvloop-0.15.2.tar.gz (2.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "  Using cached httptools-0.6.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Using cached uvicorn-0.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Using cached uvicorn-0.36.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Using cached uvicorn-0.36.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n"
     ]
    }
   ],
   "source": [
    "# !pip install openpipe-art==0.5.0 langchain-core tenacity datasets vllm faiss-cpu chromadb requests lxml numpy transformers torch gql==3.4.1 peft \n",
    "# !pip install langchain-core tenacity datasets vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965eaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from secretsConfig import oaiKey, wandbKey, openRouterKey  # Add openRouterKey\n",
    "\n",
    "# Required for RULER judge model\n",
    "os.environ[\"OPENAI_API_KEY\"] = oaiKey\n",
    "\n",
    "# Required for Weights & Biases\n",
    "os.environ[\"WANDB_API_KEY\"] = wandbKey\n",
    "\n",
    "# Required for OpenRouter (Gemini judge)\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = openRouterKey  # ADD THIS LINE\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY is required for RULER functionality.\")\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY is required for W&B.\")\n",
    "\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    raise ValueError(\"OPENROUTER_API_KEY is required for Gemini judge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c3f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag_tools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msemantic_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISSSemanticSearch\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag_tools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeyword_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keyword_search\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag_tools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mread_document\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_document_part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/main_dir/Reply-AIM-Hackathon/rag_tools/semantic_search.py:14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03msemantic_search.py - FAISS semantic search WITHOUT Ollama\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03mFor GPU support, use faiss-gpu instead of faiss-cpu\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01metree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mElementTree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mET\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlite3\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "from rag_tools.semantic_search import FAISSSemanticSearch\n",
    "from rag_tools.keyword_search import keyword_search\n",
    "from rag_tools.read_document import read_document_part\n",
    "\n",
    "# Wrap tools with error handling that tracks mistakes\n",
    "class ToolError:\n",
    "    \"\"\"Marker for tool execution errors\"\"\"\n",
    "    def __init__(self, message: str):\n",
    "        self.message = message\n",
    "\n",
    "_original_keyword_search = keyword_search\n",
    "_original_read_document_part = read_document_part\n",
    "\n",
    "def keyword_search_wrapped(query: str, num: int = 5) -> str:\n",
    "    \"\"\"Safe keyword search wrapper\"\"\"\n",
    "    try:\n",
    "        return _original_keyword_search(\"./data/normalized_enhanced.xml\", query, num)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[TOOL ERROR] keyword_search failed: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "def read_document_part_wrapped(part_id: str) -> str:\n",
    "    \"\"\"Safe read_document_part wrapper - validates part_id format\"\"\"\n",
    "    try:\n",
    "        if \" \" in part_id or len(part_id) > 100:\n",
    "            error_msg = f\"[INVALID PART_ID] '{part_id[:50]}...' is not a valid part_id format.\"\n",
    "            print(f\"[ERROR] Model tried to read invalid part_id: {part_id[:50]}...\")\n",
    "            return error_msg\n",
    "\n",
    "        return _original_read_document_part(\"./data/normalized_enhanced.xml\", part_id)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[TOOL ERROR] Failed to read document part: {str(e)}\"\n",
    "        print(f\"[ERROR] {error_msg}\")\n",
    "        return error_msg\n",
    "\n",
    "# Override with wrapped versions\n",
    "keyword_search = keyword_search_wrapped\n",
    "read_document_part = read_document_part_wrapped\n",
    "\n",
    "print(\" Tools wrapped with error handling and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "import art\n",
    "from art.serverless.backend import ServerlessBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Declare the model - CHANGED TO QWEN3-14B\n",
    "model = art.TrainableModel(\n",
    "    name=\"legal-agent-001\",\n",
    "    project=\"legal-rag\",\n",
    "    base_model=\"Qwen/Qwen2.5-14B-Instruct\",  # Changed from Qwen2.5-14B-Instruct\n",
    ")\n",
    "\n",
    "# Initialize the server\n",
    "# Training and inference will run on Weights & Biases servers\n",
    "backend = ServerlessBackend()\n",
    "\n",
    "# Register the model with the Serverless Backend (sets up logging, inference, and training)\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rollout function defined!\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "import art\n",
    "\n",
    "MAX_TURNS = 4\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: str\n",
    "    source_ids: list[str]\n",
    "\n",
    "\n",
    "class LegalScenario(BaseModel):\n",
    "    id: str\n",
    "    question: str\n",
    "    gold_answer: str | None = None\n",
    "    gold_part_ids: list[str] | None = None\n",
    "\n",
    "\n",
    "class LegalScenarioStep(BaseModel):\n",
    "    step: int\n",
    "    scenario: LegalScenario\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, legal_scenario_step: LegalScenarioStep) -> art.Trajectory:\n",
    "    \"\"\"Execute one trajectory rollout\"\"\"\n",
    "    scenario = legal_scenario_step.scenario\n",
    "    \n",
    "    traj = art.Trajectory(\n",
    "        reward=0.0,\n",
    "        messages_and_choices=[],\n",
    "        metadata={\"scenario_id\": scenario.id, \"step\": legal_scenario_step.step},\n",
    "    )\n",
    "\n",
    "    # YOUR CUSTOM PROMPT HERE\n",
    "    system_prompt = dedent(\n",
    "        f\"\"\"\n",
    "        You are a legal research assistant that can search legal documents to answer questions.\n",
    "\n",
    "        You have access to the following tools:\n",
    "\n",
    "        - search_keyword(query: str, num: int) -> str: Search using keyword/BM25 search for exact term matches.\n",
    "        - search_semantic(query: str, num: int) -> str: Search using semantic/vector search for conceptual similarity.\n",
    "        - read_document_part(part_id: str) -> str: Read a document part by ID. Part IDs use hierarchical format (e.g., A:B:C). To access parent parts, remove the last segment (e.g., A:B:C  parent is A:B).\n",
    "\n",
    "        You may call one tool per turn, for up to {MAX_TURNS} turns, before giving your final answer.\n",
    "\n",
    "        In each turn, you should analyze what information you need and respond with EITHER a tool call OR your final answer.\n",
    "\n",
    "        For tool calls, use this format:\n",
    "        <think>\n",
    "        [your reasoning for what to search for and why]\n",
    "        </think>\n",
    "        <tool>\n",
    "        {{\"name\": \"tool_name\", \"args\": {{\"query\": \"search query\"}}}}\n",
    "        </tool>\n",
    "\n",
    "        When you have enough information, give your final answer in this format:\n",
    "\n",
    "        <think>\n",
    "        [your reasoning for the answer]\n",
    "        </think>\n",
    "        <answer>\n",
    "        [your comprehensive answer citing the evidence you found or \"I don't know\" if you didn't get enough information]\n",
    "\n",
    "        <sources>\n",
    "        <source>doc_id_1</source>\n",
    "        </sources>\n",
    "        </answer>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    traj.messages_and_choices = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": scenario.question},\n",
    "    ]\n",
    "\n",
    "    # Define tools\n",
    "    def search_keyword_tool(query: str, num: int = 5) -> str:\n",
    "        return keyword_search(query, num)\n",
    "\n",
    "    def search_semantic_tool(query: str, num: int = 5) -> str:\n",
    "        searcher = FAISSSemanticSearch()\n",
    "        return searcher.search(query, num)\n",
    "\n",
    "    def read_document_part_tool(part_id: str) -> str:\n",
    "        return read_document_part(part_id)\n",
    "\n",
    "    def return_final_answer(answer: str, source_ids: list[str]) -> FinalAnswer:\n",
    "        return FinalAnswer(answer=answer, source_ids=source_ids)\n",
    "\n",
    "    tools = [search_keyword_tool, search_semantic_tool, read_document_part_tool, return_final_answer]\n",
    "    tools_by_name = {t.__name__: t for t in tools}\n",
    "    traj.tools = [convert_to_openai_tool(t) for t in tools]\n",
    "\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=model.inference_base_url,\n",
    "        api_key=model.inference_api_key,\n",
    "    )\n",
    "\n",
    "    for _ in range(MAX_TURNS):\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model.get_inference_name(),\n",
    "            temperature=1,\n",
    "            messages=traj.messages(),\n",
    "            tools=traj.tools,\n",
    "        )\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        traj.messages_and_choices.append(response.choices[0])\n",
    "\n",
    "        if not response_message.tool_calls:\n",
    "            return traj\n",
    "\n",
    "        try:\n",
    "            for tool_call in response_message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                if tool_name in tools_by_name:\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "                    result = tools_by_name[tool_name](**tool_args)\n",
    "                    traj.messages_and_choices.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(result),\n",
    "                    })\n",
    "\n",
    "                    if tool_name == \"return_final_answer\":\n",
    "                        return traj\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return traj\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "print(\" Rollout function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from litellm import acompletion\n",
    "\n",
    "# Load your training data\n",
    "DATA_FILE = \"./data/snippet_data.json\"\n",
    "\n",
    "print(f\"Loading data from {DATA_FILE}...\")\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to LegalScenario objects\n",
    "training_scenarios = []\n",
    "for item in data.get(\"items\", []):\n",
    "    for row in item.get(\"rows\", []):\n",
    "        sources = row.get(\"sources\", [])\n",
    "        gold_part_ids = sources if sources else []\n",
    "        \n",
    "        training_scenarios.append(\n",
    "            LegalScenario(\n",
    "                id=str(row[\"row_index\"]),\n",
    "                question=row[\"question\"],\n",
    "                gold_answer=row.get(\"model_answer\", \"\"),\n",
    "                gold_part_ids=gold_part_ids\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\" Loaded {len(training_scenarios)} scenarios\")\n",
    "\n",
    "\n",
    "# Custom RULER function using OpenRouter with detailed reward criteria\n",
    "async def gemini_ruler_score_group(group: art.TrajectoryGroup) -> art.TrajectoryGroup:\n",
    "      \"\"\"\n",
    "      Score trajectories using Gemini 2.5 Flash via OpenRouter\n",
    "\n",
    "      NEW SCORING CRITERIA:\n",
    "      - Correct answer: 1.0 to 2.0 points\n",
    "      - \"I don't know\" (appropriate): 0.0 to 1.0 points\n",
    "      - Wrong answer (hallucination): -1.0 to 0.0 points\n",
    "      - Format errors (tool misuse): -2.0 to -1.0 points\n",
    "\n",
    "      Bonuses:\n",
    "      - Fewer turns: +0.1 to +0.2\n",
    "      - Fewer searches: +0.1\n",
    "\n",
    "      Key: \"I don't know\" > wrong answer (penalizes hallucination)\n",
    "      \"\"\"\n",
    "\n",
    "      trajectories = group.trajectories\n",
    "      if len(trajectories) <= 1:\n",
    "          for traj in trajectories:\n",
    "              traj.reward = 0.0\n",
    "          return group\n",
    "\n",
    "      # Analyze each trajectory for tool usage and format errors\n",
    "      trajectory_analyses = []\n",
    "      for i, traj in enumerate(trajectories):\n",
    "          messages = traj.messages()\n",
    "          final_answer = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "\n",
    "          # Analyze tool usage\n",
    "          format_errors = []\n",
    "          num_searches = 0\n",
    "          num_turns = 0\n",
    "\n",
    "          for msg in messages:\n",
    "              if isinstance(msg, dict):\n",
    "                  if msg.get(\"role\") == \"assistant\":\n",
    "                      num_turns += 1\n",
    "\n",
    "                  if msg.get(\"role\") == \"tool\":\n",
    "                      tool_name = msg.get(\"name\", \"\")\n",
    "                      content = msg.get(\"content\", \"\")\n",
    "\n",
    "                      if \"search\" in tool_name.lower():\n",
    "                          num_searches += 1\n",
    "\n",
    "                      # Detect format errors\n",
    "                      if \"[TOOL ERROR]\" in content or \"[INVALID PART_ID]\" in content or \"[PART NOT FOUND]\" in content:\n",
    "                          format_errors.append(content[:80])\n",
    "\n",
    "          trajectory_analyses.append({\n",
    "              \"index\": i,\n",
    "              \"final_answer\": final_answer[:400],\n",
    "              \"num_turns\": num_turns,\n",
    "              \"num_searches\": num_searches,\n",
    "              \"has_format_error\": len(format_errors) > 0,\n",
    "              \"format_errors\": format_errors\n",
    "          })\n",
    "\n",
    "      # Build prompt for Gemini judge\n",
    "      analysis_text = \"\"\n",
    "      for analysis in trajectory_analyses:\n",
    "          analysis_text += f\"\"\"\n",
    "  **Response {analysis['index'] + 1}:**\n",
    "  - Answer: {analysis['final_answer']}\n",
    "  - Turns: {analysis['num_turns']}/{MAX_TURNS}\n",
    "  - Searches: {analysis['num_searches']}\n",
    "  - Format Errors: {\"YES\" if analysis['has_format_error'] else \"NO\"}\n",
    "  \"\"\"\n",
    "\n",
    "      judge_prompt = f\"\"\"Evaluate these {len(trajectories)} legal\n",
    "  research responses.\n",
    "\n",
    "  SCORING RULES:\n",
    "  1. Correct answer with citations: 1.0 to 2.0 points\n",
    "  2. \"I don't know\" when uncertain (GOOD - avoids hallucination): 0.0\n",
    "  to 1.0 points\n",
    "  3. Wrong/hallucinated answer: -1.0 to 0.0 points\n",
    "  4. Format errors (tool misuse): -2.0 to -1.0 points\n",
    "\n",
    "  CRITICAL: \"I don't know\" is BETTER than wrong answer. Penalize\n",
    "  hallucination heavily.\n",
    "\n",
    "  Responses:\n",
    "  {analysis_text}\n",
    "\n",
    "  Return JSON:\n",
    "  {{\n",
    "    \"scores\": [\n",
    "      {{\"base_score\": 1.5, \"reasoning\": \"why\"}},\n",
    "      {{\"base_score\": 0.5, \"reasoning\": \"why\"}},\n",
    "      {{\"base_score\": -0.5, \"reasoning\": \"why\"}}\n",
    "    ]\n",
    "  }}\n",
    "\n",
    "  Evaluation:\"\"\"\n",
    "\n",
    "      try:\n",
    "          # Call Gemini\n",
    "          response = await acompletion(\n",
    "              model=\"openrouter/google/gemini-2.5-flash\",\n",
    "              messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "              api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "              max_tokens=500,\n",
    "          )\n",
    "\n",
    "          result_text = response.choices[0].message.content.strip()\n",
    "\n",
    "          # Parse JSON\n",
    "          import re\n",
    "          json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
    "          if json_match:\n",
    "              result = json.loads(json_match.group())\n",
    "              base_scores = [item[\"base_score\"] for item in result[\"scores\"]]\n",
    "          else:\n",
    "              # Fallback: simple array\n",
    "              json_match = re.search(r'\\[[\\d\\.,\\s-]+\\]', result_text)\n",
    "              base_scores = json.loads(json_match.group()) if json_match else [0.0] * len(trajectories)\n",
    "\n",
    "          # Apply efficiency bonuses\n",
    "          final_scores = []\n",
    "          for base_score, analysis in zip(base_scores, trajectory_analyses):\n",
    "              score = float(base_score)\n",
    "\n",
    "              # Only give bonuses if no format errors\n",
    "              if not analysis['has_format_error']:\n",
    "                  # Fewer turns bonus\n",
    "                  turn_efficiency = (MAX_TURNS - analysis['num_turns']) / MAX_TURNS\n",
    "                  score += turn_efficiency * 0.2  # Up to +0.2\n",
    "\n",
    "                  # Fewer searches bonus\n",
    "                  if analysis['num_searches'] <= 2:\n",
    "                      score += 0.1\n",
    "\n",
    "              final_scores.append(round(score, 2))\n",
    "\n",
    "          # Assign scores\n",
    "          for traj, score in zip(trajectories, final_scores):\n",
    "              traj.reward = float(score)\n",
    "\n",
    "          print(f\"  Base: {[f'{s:.2f}' for s in base_scores]}  Final: {[f'{s:.2f}' for s in final_scores]}\")\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"  Judge error: {e}\")\n",
    "          # Fallback: penalize format errors\n",
    "          for traj, analysis in zip(trajectories, trajectory_analyses):\n",
    "              traj.reward = -1.5 if analysis['has_format_error'] else 0.5\n",
    "\n",
    "      return group\n",
    "\n",
    "\n",
    "# Test the NEW judge\n",
    "print(\"\\n Testing NEW scoring criteria...\")\n",
    "\n",
    "test_scenario = training_scenarios[0]\n",
    "base_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a legal research agent.\"},\n",
    "    {\"role\": \"user\", \"content\": test_scenario.question},\n",
    "]\n",
    "\n",
    "# Test case 1: Good answer\n",
    "good_traj = art.Trajectory(\n",
    "    messages_and_choices=[\n",
    "        *base_messages,\n",
    "        {\"role\": \"assistant\", \"content\": test_scenario.gold_answer},\n",
    "    ],\n",
    "    reward=0,\n",
    ")\n",
    "\n",
    "# Test case 2: \"I don't know\" (should score higher than wrong answer)\n",
    "idk_traj = art.Trajectory(\n",
    "    messages_and_choices=[\n",
    "        *base_messages,\n",
    "        {\"role\": \"assistant\", \"content\": \"I don't know - I couldn't find sufficient information to answer confidently.\"},\n",
    "    ],\n",
    "    reward=0,\n",
    ")\n",
    "\n",
    "# Test case 3: Hallucinated wrong answer (should score lowest)\n",
    "wrong_traj = art.Trajectory(\n",
    "    messages_and_choices=[\n",
    "        *base_messages,\n",
    "        {\"role\": \"assistant\", \"content\": \"The Supreme Court ruled that all contracts are void under maritime law based on Article 5.\"},\n",
    "    ],\n",
    "    reward=0,\n",
    ")\n",
    "\n",
    "test_group = art.TrajectoryGroup(trajectories=[good_traj, idk_traj, wrong_traj])\n",
    "judged_group = await gemini_ruler_score_group(test_group)\n",
    "\n",
    "# Display results\n",
    "sorted_trajs = sorted(judged_group.trajectories, key=lambda t: t.reward, reverse=True)\n",
    "print(\"\\n Ranking:\")\n",
    "for rank, traj in enumerate(sorted_trajs, 1):\n",
    "    msgs = traj.messages()\n",
    "    print(f\"  {rank}. Score {traj.reward:.2f} - {msgs[-1]['content'][:60]}...\")\n",
    "\n",
    "print(\"\\n New scoring criteria working! ('I don't know' should rank above wrong answer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshng2025\u001b[0m (\u001b[33mImperial-College-London-SPQR\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/maindir/wandb/run-20251012_003057-6q6qwhwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv' target=\"_blank\">honest-plasma-9</a></strong> to <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting training loop...\n",
      "\n",
      " Model inference running on: W&B Serverless (not your A100s)\n",
      " W&B Dashboard: https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv\n",
      "  Run name: honest-plasma-9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating dataset:   0%|          | 0/150 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 0 | Epoch 0 | Epoch Step 0 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering trajectories: 100%|| 12/12 [00:02<00:00,  5.55it/s, reward=0, completion_tokens=83.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores: [1.5, 1.5, 1.5, 1.5, 1.5, 0.5]\n",
      "  Scores: [0.5, 1.0, 1.5, 0.0, 1.0, 1.0]\n",
      "  Rewards: avg=1.083, max=1.500, min=0.000\n",
      "  Correct: 9, IDK: 3, Wrong: 0, Format Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 2/2 [00:16<00:00,  8.33s/it, entropy=0.352, grad_norm=0.659, loss=0.361, policy_loss=0.361]\n",
      "Iterating dataset:   1%|          | 1/150 [00:24<1:01:18, 24.69s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 0 complete\n",
      "\n",
      "=== Step 1 | Epoch 0 | Epoch Step 1 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering trajectories:  83%| | 10/12 [00:04<00:00,  2.32it/s, exceptions=2, reward=0, completion_tokens=84.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores: [1.0, 1.0, 1.5, 1.5, 2.0, 1.0]\n",
      "  Scores: [1.5, 1.8, 2.0, 1.0]\n",
      "  Rewards: avg=1.430, max=2.000, min=1.000\n",
      "  Correct: 10, IDK: 0, Wrong: 0, Format Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 2/2 [00:17<00:00,  8.64s/it, entropy=0.263, grad_norm=0.194, loss=-0.153, policy_loss=-0.153]\n",
      "Iterating dataset:   1%|         | 2/150 [00:55<1:10:08, 28.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1 complete\n",
      "\n",
      "=== Step 2 | Epoch 0 | Epoch Step 2 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering trajectories:  83%| | 10/12 [00:03<00:00,  2.68it/s, exceptions=2, reward=0, completion_tokens=58.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores: [0.5, 2.0, 0.0, 0.5]\n",
      "  Scores: [0.5, 0.8, 1.2, 1.0, 1.5, 1.8]\n",
      "  Rewards: avg=0.980, max=2.000, min=0.000\n",
      "  Correct: 5, IDK: 5, Wrong: 0, Format Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from art.utils import iterate_dataset\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize W&B with auto-generated run name\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "run = wandb.init(\n",
    "    project=\"IBM-Datathon-Z-2025\",\n",
    "    config={\n",
    "        \"model\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "        \"groups_per_step\": 2,\n",
    "        \"num_epochs\": 3,\n",
    "        \"rollouts_per_group\": 6,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_steps\": 50,\n",
    "        \"max_turns\": MAX_TURNS,\n",
    "    },\n",
    "    # Remove the name parameter to get auto-generated names with numbers\n",
    "    # name=\"legal-rag-rl-training\"  # REMOVED THIS LINE\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_config = run.config\n",
    "\n",
    "# Create training iterator starting from step 0 (fresh run)\n",
    "training_iterator = iterate_dataset(\n",
    "    training_scenarios,\n",
    "    groups_per_step=training_config[\"groups_per_step\"],\n",
    "    num_epochs=training_config[\"num_epochs\"],\n",
    "    initial_step=0,  # CHANGED: Always start from 0 for fresh runs\n",
    ")\n",
    "\n",
    "print(\" Starting training loop...\\n\")\n",
    "print(f\" Model inference running on: W&B Serverless (not your A100s)\")\n",
    "print(f\" W&B Dashboard: {run.url}\")\n",
    "print(f\"  Run name: {run.name}\\n\")\n",
    "\n",
    "step_count = 0\n",
    "\n",
    "for batch in training_iterator:\n",
    "    print(f\"=== Step {batch.step} | Epoch {batch.epoch} | Epoch Step {batch.epoch_step} ===\")\n",
    "    print(f\"Batch: {len(batch.items)} scenarios\")\n",
    "    \n",
    "    # Create trajectory groups\n",
    "    groups = []\n",
    "    for scenario in batch.items:\n",
    "        groups.append(\n",
    "            art.TrajectoryGroup(\n",
    "                (\n",
    "                    rollout(model, LegalScenarioStep(step=batch.step, scenario=scenario))\n",
    "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Gather trajectories\n",
    "    finished_groups = await art.gather_trajectory_groups(\n",
    "        groups,\n",
    "        pbar_desc=\"Gathering trajectories\",\n",
    "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
    "    )\n",
    "    \n",
    "    # Judge with custom Gemini function\n",
    "    judged_groups = []\n",
    "    for group in finished_groups:\n",
    "        judged_group = await gemini_ruler_score_group(group)\n",
    "        judged_groups.append(judged_group)\n",
    "    \n",
    "    # Calculate metrics before training\n",
    "    all_rewards = [t.reward for g in judged_groups for t in g.trajectories]\n",
    "    avg_reward = sum(all_rewards) / len(all_rewards)\n",
    "    max_reward = max(all_rewards)\n",
    "    min_reward = min(all_rewards)\n",
    "    \n",
    "    # Count trajectories by reward band\n",
    "    correct_count = sum(1 for r in all_rewards if r >= 1.0)\n",
    "    idk_count = sum(1 for r in all_rewards if 0.0 <= r < 1.0)\n",
    "    wrong_count = sum(1 for r in all_rewards if -1.0 <= r < 0.0)\n",
    "    format_error_count = sum(1 for r in all_rewards if r < -1.0)\n",
    "    \n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"step\": step_count,  # Use step_count instead of batch.step\n",
    "        \"epoch\": batch.epoch,\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"max_reward\": max_reward,\n",
    "        \"min_reward\": min_reward,\n",
    "        \"correct_count\": correct_count,\n",
    "        \"idk_count\": idk_count,\n",
    "        \"wrong_count\": wrong_count,\n",
    "        \"format_error_count\": format_error_count,\n",
    "        \"total_trajectories\": len(all_rewards),\n",
    "    })\n",
    "    \n",
    "    print(f\"  Rewards: avg={avg_reward:.3f}, max={max_reward:.3f}, min={min_reward:.3f}\")\n",
    "    print(f\"  Correct: {correct_count}, IDK: {idk_count}, Wrong: {wrong_count}, Format Errors: {format_error_count}\")\n",
    "    \n",
    "    # Train on judged trajectories\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(\n",
    "        judged_groups,\n",
    "        config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
    "    )\n",
    "    \n",
    "    print(f\" Step {step_count} complete\\n\")\n",
    "    \n",
    "    step_count += 1\n",
    "    \n",
    "    # Stop after max_steps\n",
    "    if step_count >= training_config[\"max_steps\"]:\n",
    "        break\n",
    "\n",
    "run.finish()\n",
    "print(\" Training complete!\")\n",
    "print(f\" View results: {run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b06c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
