{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff36f14",
   "metadata": {},
   "source": [
    "the main difference is that this operates on normalized_enhanced.xml (by Ryan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f653c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb694bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "INPUT_XML = \"normalized_enhanced.xml\"\n",
    "OUTPUT_DIR = \"usc_cases_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OPENROUTER_API_KEY = keys.OPENROUTER_KEY  # ← Insert key\n",
    "\n",
    "GENERATION_MODEL = \"google/gemini-2.5-pro\"\n",
    "FILTER_MODEL      = \"google/gemini-2.5-flash\"\n",
    "\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6586ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# OpenRouter request wrapper\n",
    "# ------------------------------\n",
    "def openrouter_request(model, prompt, temperature=TEMPERATURE):\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, headers=headers, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8481efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# XML helpers\n",
    "# ------------------------------\n",
    "def pretty_xml(elem):\n",
    "    from xml.dom import minidom\n",
    "    xml_str = ET.tostring(elem, encoding=\"utf-8\")\n",
    "    return minidom.parseString(xml_str).toprettyxml(indent=\"  \")\n",
    "\n",
    "\n",
    "def clean_json(raw):\n",
    "    \"\"\"Strip markdown/code fences/etc. to recover JSON.\"\"\"\n",
    "    raw = raw.strip().strip(\"`\")\n",
    "    start = raw.find(\"{\")\n",
    "    return raw[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c561935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CASE TEXT EXTRACTION\n",
    "# ------------------------------\n",
    "def extract_case_text(case_elem):\n",
    "    \"\"\"Concatenate all relevant text fields for grounding the LLM.\"\"\"\n",
    "    parts = []\n",
    "\n",
    "    for tag in [\"Metadata\", \"Facts\", \"Issues\", \"Holding\", \"Decision\"]:\n",
    "        section = case_elem.find(tag)\n",
    "        if section is None:\n",
    "            continue\n",
    "\n",
    "        txt = \" \".join(\n",
    "            p.text for p in section.iter() if p.text\n",
    "        )\n",
    "        if txt.strip():\n",
    "            parts.append(f\"{tag.upper()}:\\n{txt}\")\n",
    "\n",
    "    return \"\\n\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb08483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# LLM PROMPTS\n",
    "# ------------------------------\n",
    "\n",
    "def generation_prompt(context_text):\n",
    "    return f\"\"\"\n",
    "You are an expert legal analyst.\n",
    "\n",
    "Generate *exactly 10* high-quality, professional-grade,\n",
    "difficult, varied Question/Answer pairs.\n",
    "\n",
    "RULES:\n",
    "- 100% grounded in the case text\n",
    "- No external knowledge\n",
    "- Lawyer-level realism\n",
    "- Difficult questions requiring synthesis\n",
    "- Varied: facts, reasoning, issues, holding, procedure, etc.\n",
    "- Output ONLY valid JSON:\n",
    "{{\n",
    "  \"qa_pairs\": [\n",
    "    {{\"question\": \"...\", \"answer\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "CASE TEXT:\n",
    "=====================\n",
    "{context_text}\n",
    "=====================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ac84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prompt(raw_json_text):\n",
    "    return f\"\"\"\n",
    "You are a legal question quality-control judge.\n",
    "\n",
    "Your task is to FILTER and IMPROVE the Q&A pairs.\n",
    "\n",
    "Rules:\n",
    "1. Keep ONLY realistic, lawyer-level professional questions.\n",
    "2. Must require reasoning/synthesis, not trivia.\n",
    "3. Must cover varied aspects: facts, procedural posture,\n",
    "   issues, holding, reasoning, consequences.\n",
    "4. Enforce ZERO hallucination: 100% grounded in source.\n",
    "5. Ensure exactly 10 Q&A pairs.\n",
    "6. Output ONLY valid JSON in this structure:\n",
    "\n",
    "{{\n",
    "  \"qa_pairs\": [\n",
    "    {{\"question\": \"...\", \"answer\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "INPUT QAs:\n",
    "=====================\n",
    "{raw_json_text}\n",
    "=====================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b43e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# GENERATION PIPELINE\n",
    "# ------------------------------\n",
    "\n",
    "def generate_qa_pairs(context_text):\n",
    "    # Stage 1 → generate raw pairs\n",
    "    raw = openrouter_request(\n",
    "        GENERATION_MODEL,\n",
    "        generation_prompt(context_text)\n",
    "    )\n",
    "\n",
    "    # Stage 2 → filter/refine/validate\n",
    "    filtered = openrouter_request(\n",
    "        FILTER_MODEL,\n",
    "        filter_prompt(raw)\n",
    "    )\n",
    "\n",
    "    # Parse JSON result\n",
    "    cleaned = clean_json(filtered)\n",
    "    data = json.loads(cleaned)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc8630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# OUTPUT XML\n",
    "# ------------------------------\n",
    "def write_case_output(case_elem, qa_data, index):\n",
    "    case_id   = case_elem.get(\"slug\", \"UNKNOWN\")\n",
    "    case_name = case_elem.get(\"name\", \"Unknown_Case\")\n",
    "\n",
    "    root = ET.Element(\"LegalDataEngineeringOutput\")\n",
    "\n",
    "    src = ET.SubElement(root, \"SourceDataset\", {\"docId\": case_id})\n",
    "    # Embed original case XML node\n",
    "    src.append(case_elem)\n",
    "\n",
    "    qas = ET.SubElement(root, \"QAPairs\", {\n",
    "        \"caseId\": case_id,\n",
    "        \"caseName\": case_name\n",
    "    })\n",
    "\n",
    "    for pair in qa_data[\"qa_pairs\"]:\n",
    "        p = ET.SubElement(qas, \"Pair\")\n",
    "        q = ET.SubElement(p, \"Question\")\n",
    "        q.text = pair[\"question\"]\n",
    "        a = ET.SubElement(p, \"Answer\")\n",
    "        a.text = pair[\"answer\"]\n",
    "\n",
    "    # Filename: numbered + safe\n",
    "    safe_name = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", case_name)\n",
    "    filename = f\"{index:02d}_{safe_name}.xml\"\n",
    "    path = Path(OUTPUT_DIR) / filename\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pretty_xml(root))\n",
    "\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec7105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 cases\n",
      "\n",
      "Processing case 1: Fletcher v. Peck\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ------------------------------\n",
    "\n",
    "tree = ET.parse(INPUT_XML)\n",
    "root = tree.getroot()\n",
    "cases = root.findall(\"Case\")\n",
    "\n",
    "print(f\"Found {len(cases)} cases\")\n",
    "\n",
    "output_paths = []\n",
    "\n",
    "for idx, case_elem in enumerate(cases, start=1):\n",
    "    case_name = case_elem.get(\"name\")\n",
    "    print(f\"\\nProcessing case {idx}: {case_name}\")\n",
    "\n",
    "    context = extract_case_text(case_elem)\n",
    "    qa_data = generate_qa_pairs(context)\n",
    "\n",
    "    path = write_case_output(case_elem, qa_data, idx)\n",
    "    print(\" → Saved:\", path)\n",
    "\n",
    "print(\"\\nAll cases completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
